{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cPickle\n",
    "import re\n",
    "%run 'bible_data.ipynb'\n",
    "%run 'learn_tools.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible has 889970 `words`, vocab size 12483, word 202 is sorroweth, id for 'the' is 3748 \n"
     ]
    }
   ],
   "source": [
    "words, vdict, rvdict = bible_as_words_inc_punc()\n",
    "vocab_size = len(vdict)\n",
    "print \"Bible has %s `words`, vocab size %s, word %s is %s, id for '%s' is %s \" % (\n",
    "    len(words), vocab_size, 202, vdict[202], 'the', rvdict['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prep Scipgram data\n",
    "X_Y = []\n",
    "skips = 1\n",
    "for i, word in enumerate(words):\n",
    "    if i < skips or i >= len(words) - skips - 1: \n",
    "        continue\n",
    "    for skip in [s for s in xrange(-skips,skips +1) if s is not 0]:\n",
    "        X_Y.append((rvdict[word], rvdict[words[i + skip]]))\n",
    "random.shuffle(X_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1778934 train examples, 1000 validation examples\n"
     ]
    }
   ],
   "source": [
    "num_validation = 1000\n",
    "val_X_Y= X_Y[:num_validation]\n",
    "X_Y = X_Y[num_validation:]\n",
    "print (\"%s train examples, %s validation examples\" % (len(X_Y), len(val_X_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net built!\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    num_embed_dims = 32\n",
    "    \n",
    "    # Train inputs\n",
    "    train_labels_in = tf.placeholder(tf.int64, [None], name=\"train_word_labels\")\n",
    "    train_labels = tf.reshape(train_labels_in, [-1,1], name=\"train_labels\")\n",
    "    train_word_ids = tf.placeholder(tf.int64, [None], name=\"train_word_ids\")\n",
    "    \n",
    "    # Validation inputs\n",
    "    val_labels_in = tf.placeholder(tf.int64, [None], name=\"val_word_labels\")\n",
    "    val_labels = tf.reshape(val_labels_in, [-1,1], name=\"val_labels\")\n",
    "    val_word_ids = tf.placeholder(tf.int64, [None], name=\"val_word_ids\")\n",
    "    \n",
    "    # Similarity words\n",
    "    sim_word_ids = tf.placeholder(tf.int64, [None], name=\"sim_word_ids\")\n",
    "    \n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_normal([vocab_size, num_embed_dims], 0.0, 0.1), name=\"Embeddings\")\n",
    "    w = tf.Variable(tf.random_normal([vocab_size, num_embed_dims], 0.0, 1), name=\"Output_Weight\")\n",
    "    b = tf.Variable(tf.zeros([vocab_size]), name=\"Output_Bias\")\n",
    "\n",
    "    def model(in_tensor, name=None):\n",
    "        name = \"\" if name is None else name + '_'\n",
    "        in_embeddings = tf.nn.embedding_lookup(embeddings, in_tensor, name=(name+\"input_embeddings\"))\n",
    "        return in_embeddings\n",
    "    \n",
    "    train_embeddings = model(train_word_ids)\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(w, b, train_embeddings, train_labels,\n",
    "                                                     100, vocab_size, name=\"train_sampled_softmax\"),\n",
    "                          name=\"loss\")\n",
    "    train_embeddings = tf.Print(loss, [train_embeddings], 'train embeddings')\n",
    "    optimzer = tf.train.AdamOptimizer(0.003).minimize(loss)\n",
    "    \n",
    "    val_loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(w, b, model(val_word_ids), val_labels,\n",
    "                                                     100, vocab_size, name=\"val_sample_softmax\"),\n",
    "                             name=\"val_loss\")\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1, keep_dims=True))\n",
    "    norm_embeddings = embeddings/ norm\n",
    "    sim_word_embed = tf.nn.embedding_lookup(norm_embeddings, sim_word_ids)\n",
    "    similarity = tf.matmul(sim_word_embed, tf.transpose(norm_embeddings))\n",
    "    \n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "print (\"Net built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "batch_size = 64\n",
    "train_steps = 21\n",
    "eval_at = 1\n",
    "summary = 20\n",
    "history = []\n",
    "load_from = None\n",
    "save_at = \"remote.small.bible.ckpt\" # \"bible_skip_gram.retrain.ckpt\"\n",
    "#with tf.Session(\"grpc://52.211.134.160:2222\", graph=graph) as sess:\n",
    "with tf.Session(graph=graph) as sess:\n",
    "# cluster = tf.train.ClusterSpec({\"local\": [\"52.210.227.65:2222\"]})\n",
    "# server = tf.train.Server(cluster, job_name=\"local\", task_index=0)\n",
    "# with tf.Session(server.target,graph=graph) as sess:\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "    if load_from:\n",
    "        saver.restore(sess, load_from)\n",
    "        print (\"Loaded: %s\" % load_from)\n",
    "    \n",
    "    for step in xrange(train_steps):\n",
    "        batch_x, batch_y = zip(*random.sample(X_Y, batch_size))\n",
    "        l, _ = sess.run([loss, optimzer], feed_dict={train_word_ids: batch_x, train_labels_in: batch_y})\n",
    "        history.append({'loss':l, 'step':step})\n",
    "        \n",
    "        if step % eval_at == 0:\n",
    "            val_batch_x, val_batch_y = zip(*X_Y)\n",
    "            vl = val_loss.eval({val_word_ids:val_batch_x, val_labels_in:val_batch_y})\n",
    "            history[-1].update({'val_loss':vl})\n",
    "            print (\"Validation loss: %s\" % vl)\n",
    "            \n",
    "        if step % summary == summary -1 :\n",
    "            show_learning_history(history)\n",
    "            # Save the variables to disk.\n",
    "            if save_at:\n",
    "                save_path = saver.save(sess, save_at)\n",
    "                print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "                    \n",
    "\n",
    "# Save the variables to disk.\n",
    "if save_at:\n",
    "    save_path = saver.save(sess, save_at)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "print(\"All done!\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: bible_skip_gram.retrain.ckpt\n",
      "pickled to bible.learned2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Save weights and dicts\n",
    "load_from = \"bible_skip_gram.retrain.ckpt\"\n",
    "pickleto = \"bible.learned2.pickle\"\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(sess, load_from)\n",
    "    print (\"Loaded: %s\" % load_from)\n",
    "    ebed_arr = embeddings.eval()\n",
    "\n",
    "    \n",
    "with open(pickleto, 'w') as fp:\n",
    "    cPickle.dump({\n",
    "        'embeddings': ebed_arr,\n",
    "        'vdict' : vdict,\n",
    "        'rvdict' : rvdict\n",
    "    },fp)\n",
    "print (\"pickled to %s\" % pickleto)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: bible_skip_gram.retrain.ckpt\n",
      "shape sim = (22, 12483)\n",
      "Nearest to and: amen, but, dumah, slip,\n",
      "Nearest to moved: sojourn, posterity, ramah, saints,\n",
      "Nearest to deep: idolatries, oblations, cyrene, sticks,\n",
      "Nearest to in: with, perez, stiffnecked, impoverish,\n",
      "Nearest to earth: heavens, amariah, mesha, thirst,\n",
      "Nearest to darkness: islands, flatteries, jesaiah, unbelief,\n",
      "Nearest to god: straits, housetops, woe, beautiful,\n",
      "Nearest to ,: ;, ?, ., glorifieth,\n",
      "Nearest to .: ;, ?, :, ,,\n",
      "Nearest to ;: :, ,, ., ?,\n",
      "Nearest to was: is, fifteenth, further, had,\n",
      "Nearest to form: increase, saving, shephatiah, cake,\n",
      "Nearest to void: visit, rulest, conception, restore,\n",
      "Nearest to upon: with, against, toward, by,\n",
      "Nearest to beginning: evidence, act, cool, overlaying,\n",
      "Nearest to spirit: prize, murmur, cart, ahasbai,\n",
      "Nearest to heaven: judaea, capernaum, prison, mischief,\n",
      "Nearest to created: suffered, congregation, appear, mesopotamia,\n",
      "Nearest to of: suborned, scaleth, pare, distant,\n",
      "Nearest to face: mouth, belly, seed, neck,\n",
      "Nearest to without: encouraged, beholdeth, breakest, restoreth,\n",
      "Nearest to the: thy, destroyeth, journeying, stealth,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "orig:\n",
      "  unto moses in the land of egypt, that the lord spake unto moses, saying, i am the lord:\n",
      "new:\n",
      "  unto moses in the land of truth, that the lord revolted unto saul? saying. i am the lord:\n",
      "\n",
      "orig:\n",
      "  unto the tent; and, behold, it was hid in his tent, and the silver under it. and\n",
      "new:\n",
      "  unto the tent; and, behold, he was hid in his buzi, and the silver without it. and\n",
      "\n",
      "orig:\n",
      "  eyes: behold, i will engrave the graving thereof, saith the lord of hosts, and i will remove the\n",
      "new:\n",
      "  eyes; nepheg, i will engrave the jotham thereof, saith the lord pare flocks; and ye might remove the\n",
      "\n",
      "orig:\n",
      "  their unrighteousness, and their sins and their iniquities will i remember no more. in that he saith, a new\n",
      "new:\n",
      "  their unrighteousness, dumah their sins and their iniquities shall ye remember no more. in that they saith, a new\n",
      "\n",
      "orig:\n",
      "  he was extolled with my tongue. if i regard iniquity in my heart, the lord will not hear me:\n",
      "new:\n",
      "  he was extolled with my tongue. but i regard iniquity with my heart, the lord will not hear me.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_from = \"bible_skip_gram.retrain.ckpt\"\n",
    "top_n = 8\n",
    "\n",
    "def theosaurus(word):\n",
    "    ids = [rvdict[word]]\n",
    "    sim = similarity.eval({sim_word_ids:ids})\n",
    "    nearest = (-sim[0, :]).argsort()[1:3+1] # top 4 matches\n",
    "    return vdict[random.choice(nearest)]\n",
    "\n",
    "def print_nearest(words, top_n):\n",
    "    word_ids = [rvdict[wrd] for wrd in words]\n",
    "    sim = similarity.eval({sim_word_ids:word_ids})\n",
    "    print(\"shape sim = %s\" % (sim.shape,))\n",
    "    for i, wid in enumerate(word_ids):\n",
    "        word = vdict[wid]\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_n+1]\n",
    "        log = 'Nearest to %s:' % word\n",
    "        for n in range(top_n):\n",
    "          close_word = vdict[nearest[n]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(sess, load_from)\n",
    "    print (\"Loaded: %s\" % load_from)\n",
    "    \n",
    "    print_nearest(set(words[:40]), 4)\n",
    "    \n",
    "    print ('\\n'*3)\n",
    "    punc_reg = re.compile(r\" ([.-_,:';?])\")\n",
    "    for _ in xrange(5):\n",
    "        sentance_length = 22\n",
    "        ofset = random.randint(sentance_length, len(words) - sentance_length) \n",
    "        sentance = words[ ofset:ofset + sentance_length]\n",
    "        new_sentance = [word if random.random() < 0.8 else theosaurus(word) for i, word in enumerate(sentance)]\n",
    "        print \"orig:\"\n",
    "        print (punc_reg.sub(r'\\1',  '  '+' '.join(sentance)))\n",
    "        print \"new:\"\n",
    "        print (punc_reg.sub(r'\\1',  '  '+' '.join(new_sentance)))\n",
    "        print('')\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
